{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdamVinestock/NLP/blob/main/NLP_Language_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ce5pQK3bFn_"
      },
      "source": [
        "# Language Models\n",
        "In this notebook we will be creating tools for learning and testing language models.\n",
        "The corpora that we will be working with are lists of tweets in 8 different languages that use the Latin script. The data is provided either formatted as CSV or as JSON, for your convenience. The end goal is to write a set of tools that can detect the language of a given tweet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwG8v-Ll49KM"
      },
      "source": [
        "*As a preparation for this task, download the data files from the course git repository.\n",
        "\n",
        "The relevant files are under **lm-languages-data-new**:\n",
        "\n",
        "\n",
        "*   en.csv (or the equivalent JSON file)\n",
        "*   es.csv (or the equivalent JSON file)\n",
        "*   fr.csv (or the equivalent JSON file)\n",
        "*   in.csv (or the equivalent JSON file)\n",
        "*   it.csv (or the equivalent JSON file)\n",
        "*   nl.csv (or the equivalent JSON file)\n",
        "*   pt.csv (or the equivalent JSON file)\n",
        "*   tl.csv (or the equivalent JSON file)\n",
        "*   test.csv (or the equivalent JSON file)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xC-87z2GWMq",
        "outputId": "a1d38dec-f8ab-4d18-8a76-4f734dc65517"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nlp-course'...\n",
            "remote: Enumerating objects: 71, done.\u001b[K\n",
            "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
            "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
            "remote: Total 71 (delta 29), reused 40 (delta 11), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (71/71), 11.28 MiB | 13.97 MiB/s, done.\n",
            "Resolving deltas: 100% (29/29), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/kfirbar/nlp-course.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOVb4IhsqimJ"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Important note: please use only the files under lm-languages-data-new and NOT under lm-languages-data**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYdhPfbAGkip",
        "outputId": "22298167-b19d-4cf1-e080-5bdadc2413f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en.csv\t es.json  in.csv   it.json  pt.csv    test.json   tl.csv\n",
            "en.json  fr.csv   in.json  nl.csv   pt.json   tests.csv   tl.json\n",
            "es.csv\t fr.json  it.csv   nl.json  test.csv  tests.json\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!ls nlp-course/lm-languages-data-new\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ashyu_mT28o6"
      },
      "source": [
        "**Part 1**\n",
        "\n",
        "We will write a function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data. **Our token definition is a single UTF-8 encoded character**. So, the vocabulary list is a simple Python list of all the characters that you see at least once in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2WV3cpE666tF"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import math\n",
        "\n",
        "en_file = open('/content/nlp-course/lm-languages-data-new/en.csv')\n",
        "es_file = open('/content/nlp-course/lm-languages-data-new/es.csv')\n",
        "fr_file = open('/content/nlp-course/lm-languages-data-new/fr.csv')\n",
        "in_file = open('/content/nlp-course/lm-languages-data-new/in.csv')\n",
        "it_file = open('/content/nlp-course/lm-languages-data-new/it.csv')\n",
        "nl_file = open('/content/nlp-course/lm-languages-data-new/nl.csv')\n",
        "pt_file = open('/content/nlp-course/lm-languages-data-new/pt.csv')\n",
        "tl_file = open('/content/nlp-course/lm-languages-data-new/tl.csv')\n",
        "\n",
        "csv_files = ['/content/nlp-course/lm-languages-data-new/en.csv',\n",
        "            '/content/nlp-course/lm-languages-data-new/es.csv',\n",
        "            '/content/nlp-course/lm-languages-data-new/fr.csv',\n",
        "            '/content/nlp-course/lm-languages-data-new/in.csv',\n",
        "            '/content/nlp-course/lm-languages-data-new/it.csv',\n",
        "            '/content/nlp-course/lm-languages-data-new/nl.csv',\n",
        "            '/content/nlp-course/lm-languages-data-new/pt.csv',\n",
        "            '/content/nlp-course/lm-languages-data-new/tl.csv']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xCfzsITW8Yaj"
      },
      "outputs": [],
      "source": [
        "def calc_vocab(csv_files):\n",
        "  vocab = [['<s>'],['<e>']]\n",
        "  for path in csv_files:\n",
        "    with open(path, 'r', newline='', encoding='utf-8') as csv_file:\n",
        "        reader = csv.reader(csv_file)\n",
        "        # Pad each tweet with start and end symbols given n\n",
        "        for row in reader:\n",
        "          for char in row[1]:\n",
        "              if [char] not in vocab:\n",
        "                vocab.append([char])\n",
        "  return vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Esm7K_A69eYF"
      },
      "outputs": [],
      "source": [
        "vocab = calc_vocab(csv_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb2PGj0Yc2TY"
      },
      "source": [
        "**Part 2**\n",
        "\n",
        "Now we will write a function lm that generates a language model from a textual corpus. The function should return a dictionary (representing a model) where the keys are all the relevant n-1 sequences, and the values are dictionaries with the n_th tokens and their corresponding probabilities to occur. For example, for a trigram model (tokens are characters), it should look something like:\n",
        "\n",
        "{\n",
        "  \"ab\":{\"c\":0.5, \"b\":0.25, \"d\":0.25},\n",
        "  \"ca\":{\"a\":0.2, \"b\":0.7, \"d\":0.1}\n",
        "}\n",
        "\n",
        "which means for example that after the sequence \"ab\", there is a 0.5 chance that \"c\" will appear, 0.25 for \"b\" to appear and 0.25 for \"d\" to appear.\n",
        "\n",
        "Note - We should think how to add the add_one smoothing information to the dictionary and implement it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wCqXupuv0dyb"
      },
      "outputs": [],
      "source": [
        "def lm(n, vocabulary, data_file_path, add_one):\n",
        "  # n - the n-gram to use (e.g., 1 - unigram, 2 - bigram, etc.)\n",
        "  # vocabulary - the vocabulary list (which you should use for calculating add_one smoothing)\n",
        "  # data_file_path - the data_file from which we record probabilities for our model\n",
        "  # add_one - True/False (use add_one smoothing or not)\n",
        "  # initialize the LM dictionary\n",
        "  lm = {}\n",
        "\n",
        "  with open(data_file_path, 'r', newline='', encoding='utf-8') as csv_file:\n",
        "\n",
        "    reader = csv.reader(csv_file)\n",
        "    updated_rows = []\n",
        "\n",
        "    # Pad each tweet with start and end symbols given n\n",
        "    for row in reader:\n",
        "      if row[1][0:2] == 'RT':                       # Clearing the \"RT\" string (respond tweet symbol) that is very abundant in the tweets\n",
        "        colon_start_index = row[1].find(':') + 2\n",
        "        row[1] = [vocabulary[0]] * (n - 1) + [[c] for c in row[1][colon_start_index:]] + [vocabulary[1]]\n",
        "      else:\n",
        "        row[1] = [vocabulary[0]] * (n - 1) + [[c] for c in row[1]] + [vocabulary[1]]\n",
        "      updated_rows.append(row)\n",
        "\n",
        "    csv_file.seek(0)  #<-- set the iterator to beginning of the input file\n",
        "\n",
        "    # iterate over all n-grams in the text\n",
        "    for row in updated_rows:\n",
        "      for i in range(len(row[1])-n+1):\n",
        "        ngram = ''.join([elem[0] for elem in row[1][i:i+n-1]])\n",
        "        next_token = row[1][i+n-1][0]\n",
        "        if ngram not in lm:\n",
        "          lm[ngram] = {}\n",
        "          lm[ngram][next_token] = 1\n",
        "        else:\n",
        "          if next_token not in  lm[ngram]:\n",
        "            lm[ngram][next_token] = 1\n",
        "          else:\n",
        "            lm[ngram][next_token] += 1\n",
        "\n",
        "    # normalize the probabilities\n",
        "    if add_one:\n",
        "      for ngram in lm:\n",
        "        total_count = sum(lm[ngram].values())\n",
        "        for token in lm[ngram]:\n",
        "          lm[ngram][token] +=1\n",
        "          lm[ngram][token] /= (total_count+len(vocabulary))\n",
        "        lm[ngram]['unseen'] = 1 / len(vocabulary)\n",
        "    else:\n",
        "      for ngram in lm:\n",
        "        lm[ngram]['unseen'] = 1e-7\n",
        "        total_count = sum(lm[ngram].values())\n",
        "        for token in lm[ngram]:\n",
        "          lm[ngram][token] /= total_count\n",
        "  return lm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M8TchtI22I3"
      },
      "source": [
        "**Part 3**\n",
        "\n",
        "Here we write a function *eval* that returns the perplexity of a model (dictionary) running over a given data file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "F0kkMn328-lJ"
      },
      "outputs": [],
      "source": [
        "def evaluate(n, model, data_file_path):\n",
        "  # n - the n-gram that you used to build your model (must be the same number)\n",
        "  # model - the dictionary (model) to use for calculating perplexity\n",
        "  # data_file - the tweets file that you wish to claculate a perplexity score for\n",
        "  perplexity = 0\n",
        "\n",
        "  with open(data_file_path, 'r', newline='', encoding='utf-8') as csv_file:\n",
        "    reader = csv.reader(csv_file)\n",
        "    # Pad each tweet with start and end symbols given n\n",
        "    start_symbol = '<s>'\n",
        "    end_symbol = '<e>'\n",
        "    for row in reader:\n",
        "      if row[1][0:2] == 'RT':                 # Clearing the \"RT\" string (respond tweet symbol) that is very abundant in the tweets\n",
        "        colon_index = row[1].find(':') + 2\n",
        "        row[1] = [start_symbol] * (n - 1) + [[c] for c in row[1][colon_index:]] + [end_symbol]\n",
        "      else:\n",
        "        row[1] = [start_symbol] * (n - 1) + [[c] for c in row[1]] + [end_symbol]\n",
        "\n",
        "    csv_file.seek(0)  #<-- set the iterator to beginning of the input file\n",
        "\n",
        "    # Calculate the perplexity of the model on the sequence of tokens\n",
        "    log_prob = 0\n",
        "    N = 0\n",
        "    for tweet in reader:\n",
        "      for i in range(n - 1, len(tweet[1])):\n",
        "        prefix = tweet[1][i - n + 1:i]\n",
        "        token = tweet[1][i]\n",
        "        if prefix in model:\n",
        "          if token in model[prefix]:\n",
        "            log_prob += math.log(model[prefix][token], 2)\n",
        "          else:\n",
        "            log_prob += math.log(model[prefix]['unseen'], 2)\n",
        "          N += 1\n",
        "        else:\n",
        "          log_prob += math.log((1e-7), 2)  # Smoothing for unknown tokens\n",
        "          N += 1\n",
        "\n",
        "    perplexity = 2 ** (-log_prob / (N - n + 1))\n",
        "  return perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-znVZSGc1tz",
        "outputId": "741ceb83-c0f6-4e54-93aa-f2285a724262"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8.488455023700967\n"
          ]
        }
      ],
      "source": [
        "model_en = lm(4, vocab, csv_files[0], add_one=False)\n",
        "perplexity = evaluate(4, model_en, csv_files[0])\n",
        "print(perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enGmtLE3921p"
      },
      "source": [
        "**Part 4**\n",
        "\n",
        "Now we write a function *match* that creates a model for every relevant language, using a specific value of *n* and *add_one*. Then, calculate the perplexity of all possible pairs (e.g., en model applied on the data files en ,es, fr, in, it, nl, pt, tl; es model applied on the data files en, es...). This function should return a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages. Then, the values are the relevant perplexity values.\n",
        "\n",
        "We will save the dataframe to a CSV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "caAxLE9s_fvn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def match(n, add_one):\n",
        "  # n - the n-gram to use for creating n-gram models\n",
        "  # add_one - use add_one smoothing or not\n",
        "  # preprocessing test set:\n",
        "\n",
        "  csv_paths = ['/content/nlp-course/lm-languages-data-new/en.csv',\n",
        "            '/content/nlp-course/lm-languages-data-new/es.csv',\n",
        "            '/content/nlp-course/lm-languages-data-new/fr.csv',\n",
        "            '/content/nlp-course/lm-languages-data-new/in.csv',\n",
        "            '/content/nlp-course/lm-languages-data-new/it.csv',\n",
        "            '/content/nlp-course/lm-languages-data-new/nl.csv',\n",
        "            '/content/nlp-course/lm-languages-data-new/pt.csv',\n",
        "            '/content/nlp-course/lm-languages-data-new/tl.csv']\n",
        "  models = []\n",
        "  vocab = calc_vocab(csv_paths)\n",
        "  data = [ [] for i in range(8) ]   # ['en', 'es', 'fr', 'in', 'it', 'nl', 'pt', 'tl']\n",
        "  for i in range(8):\n",
        "    cur_model = lm(n, vocab, csv_paths[i], add_one)\n",
        "    models.append(cur_model)\n",
        "    for j, path in enumerate (csv_paths):\n",
        "      cur_perplexity = evaluate(n, cur_model, path)\n",
        "      data[j].append(cur_perplexity)\n",
        "\n",
        "  # Creates pandas DataFrame.\n",
        "  df = pd.DataFrame(data, columns = ['en', 'es', 'fr', 'in', 'it', 'nl', 'pt', 'tl'],\n",
        "                    index=['en_model',\n",
        "                           'es_model',\n",
        "                           'fr_model',\n",
        "                           'in_model',\n",
        "                           'it_model',\n",
        "                           'nl_model',\n",
        "                           'pt_model',\n",
        "                           'tl_model'])\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8hsNL_AA9Sw",
        "outputId": "fd81fe73-97a9-451e-e0e7-0fc684fba5d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  en          es          fr          in          it  \\\n",
            "en_model   10.921999  117.729975   81.424843   77.802859  105.134105   \n",
            "es_model  111.415093   10.430856   89.893183  115.334352   79.429330   \n",
            "fr_model  156.772223  150.140266   10.258980  198.968866  136.237585   \n",
            "in_model  152.079900  235.264782  167.552912   11.236029  238.260683   \n",
            "it_model   90.942859   70.817712   76.159034  101.551387    9.872314   \n",
            "nl_model  138.161507  232.550872  150.825478  124.758122  224.727889   \n",
            "pt_model  154.440048   80.220956  119.642507  143.885979   99.142703   \n",
            "tl_model  125.758773  172.312157  153.489255   82.925432  150.868592   \n",
            "\n",
            "                  nl          pt          tl  \n",
            "en_model   63.358218  154.734100   65.383332  \n",
            "es_model  116.115543   79.403854   98.254611  \n",
            "fr_model  132.113648  200.924519  215.240327  \n",
            "in_model  126.007682  328.942501   85.703104  \n",
            "it_model  103.388319  106.398754   90.691757  \n",
            "nl_model    9.703911  319.486263  155.074650  \n",
            "pt_model  148.746978   10.482324  131.793835  \n",
            "tl_model  123.267574  208.042607   10.566797  \n"
          ]
        }
      ],
      "source": [
        "df = match(3,False)\n",
        "print(df)\n",
        "df.to_csv('part4.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waGMwA8H_n17"
      },
      "source": [
        "**Part 5**\n",
        "\n",
        "Now we run match with *n* values 1-4, once with add_one and once without, and print the 8 tables to this notebook, one after another.\n",
        "\n",
        "We will load each result to a dataframe and save to a CSV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nk32naXyAMdl"
      },
      "outputs": [],
      "source": [
        "def run_match():\n",
        "  for n in range(1,5):\n",
        "    file_name = 'part5.csv'\n",
        "    file_name_no_addone = 'no_addone_part5.csv'\n",
        "    df_True = match(n,True)\n",
        "    print(f'DataFrame of perplexity for n={n}, add_one =True:')\n",
        "    print(df_True)\n",
        "    df_True.to_csv(file_name)\n",
        "    df_False = match(n,False)\n",
        "    print(f'DataFrame of perplexity for n={n}, add_one =False:')\n",
        "    print(df_False)\n",
        "    df_False.to_csv(file_name_no_addone)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adrJDvbFvpFY",
        "outputId": "0f7e84e3-8953-49f7-ac86-1447d0e9914f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame of perplexity for n=1, add_one =True:\n",
            "                 en         es         fr         in         it         nl  \\\n",
            "en_model  37.857265  41.344437  40.819809  41.691754  40.504496  39.696012   \n",
            "es_model  38.798983  35.505579  38.546398  40.065849  37.859017  39.319225   \n",
            "fr_model  40.416394  39.317337  36.819573  45.188603  39.070071  40.715803   \n",
            "in_model  40.448256  42.850454  43.616238  36.611741  42.414282  40.354234   \n",
            "it_model  38.891335  38.844332  38.968632  41.757037  36.847002  39.703772   \n",
            "nl_model  38.507973  40.347719  39.751017  40.675883  39.874853  36.584532   \n",
            "pt_model  40.059716  37.270494  38.908847  41.754206  38.576469  40.542756   \n",
            "tl_model  43.873721  46.687285  48.668691  41.756029  45.501698  45.357936   \n",
            "\n",
            "                 pt         tl  \n",
            "en_model  42.024487  41.420112  \n",
            "es_model  36.794945  41.933332  \n",
            "fr_model  39.583712  46.071420  \n",
            "in_model  42.320793  38.048383  \n",
            "it_model  39.376133  41.439620  \n",
            "nl_model  40.618860  41.922881  \n",
            "pt_model  36.490170  43.596289  \n",
            "tl_model  46.735582  40.021205  \n",
            "DataFrame of perplexity for n=1, add_one =False:\n",
            "                 en         es         fr         in         it         nl  \\\n",
            "en_model  37.799005  42.268822  41.916510  42.528161  41.762377  40.954567   \n",
            "es_model  43.312104  35.443534  40.073841  49.998330  38.561130  41.273747   \n",
            "fr_model  46.239282  42.748345  36.764158  49.700940  41.437581  41.981197   \n",
            "in_model  41.612111  44.226675  44.910350  36.550832  43.915210  41.733242   \n",
            "it_model  42.501125  41.381044  39.926679  45.050083  36.777142  41.300899   \n",
            "nl_model  39.059047  41.291611  40.487299  41.414765  40.673991  36.524891   \n",
            "pt_model  49.303958  43.740086  44.448288  54.327919  45.309366  45.851377   \n",
            "tl_model  44.914383  47.964184  49.824238  42.613919  46.799314  46.646177   \n",
            "\n",
            "                 pt         tl  \n",
            "en_model  43.188338  42.636871  \n",
            "es_model  37.367862  43.231354  \n",
            "fr_model  41.829301  50.542101  \n",
            "in_model  43.715837  39.204755  \n",
            "it_model  42.165059  44.675249  \n",
            "nl_model  41.637017  42.628109  \n",
            "pt_model  36.408373  47.099505  \n",
            "tl_model  48.192883  39.941036  \n",
            "DataFrame of perplexity for n=2, add_one =True:\n",
            "                 en         es         fr         in         it         nl  \\\n",
            "en_model  22.340026  35.723258  31.019927  32.333726  34.983887  29.245994   \n",
            "es_model  30.137913  20.039030  26.731451  34.578876  25.971770  32.458533   \n",
            "fr_model  31.976555  31.711382  20.830580  37.163986  31.793258  32.360368   \n",
            "in_model  31.970108  37.670578  35.608936  22.174669  36.618795  32.200964   \n",
            "it_model  30.275513  27.017314  28.034346  31.756337  20.390571  32.611411   \n",
            "nl_model  29.002539  35.526093  31.378626  31.412602  35.345656  21.003236   \n",
            "pt_model  34.889468  28.336524  30.944430  38.948479  30.172482  36.487787   \n",
            "tl_model  31.801033  38.260881  38.174811  28.961526  36.521534  34.208677   \n",
            "\n",
            "                 pt         tl  \n",
            "en_model  38.037569  30.074634  \n",
            "es_model  25.571676  30.725257  \n",
            "fr_model  32.907849  37.147584  \n",
            "in_model  40.347979  27.504818  \n",
            "it_model  29.176038  30.465151  \n",
            "nl_model  38.098569  32.881624  \n",
            "pt_model  21.377231  34.629495  \n",
            "tl_model  40.942463  22.699666  \n",
            "DataFrame of perplexity for n=2, add_one =False:\n",
            "                 en         es         fr         in         it         nl  \\\n",
            "en_model  19.029300  32.104344  28.107221  28.689848  31.311670  26.339373   \n",
            "es_model  34.082565  17.011339  30.774523  38.473951  26.241989  35.279206   \n",
            "fr_model  39.410840  33.388051  17.808522  48.464769  35.122039  32.724746   \n",
            "in_model  29.050380  33.576824  32.202627  18.648418  32.802009  29.301904   \n",
            "it_model  29.611085  25.492153  26.264314  30.769487  17.269747  30.713069   \n",
            "nl_model  26.315292  31.755446  28.313954  28.127975  31.673068  18.058075   \n",
            "pt_model  43.270140  29.038694  35.470890  47.418026  33.601456  41.450090   \n",
            "tl_model  28.491075  33.422536  34.025624  25.670245  32.365946  30.833833   \n",
            "\n",
            "                 pt         tl  \n",
            "en_model  33.763130  26.678229  \n",
            "es_model  23.404190  31.609103  \n",
            "fr_model  33.317563  49.193486  \n",
            "in_model  35.926879  24.632858  \n",
            "it_model  27.688527  29.570548  \n",
            "nl_model  33.804628  29.336990  \n",
            "pt_model  17.539061  41.431235  \n",
            "tl_model  35.711851  18.746543  \n",
            "DataFrame of perplexity for n=3, add_one =True:\n",
            "                 en          es          fr          in          it  \\\n",
            "en_model  30.007539   87.646631   69.141866   74.380137   83.841128   \n",
            "es_model  77.722191   27.796572   64.726826   93.605228   58.327550   \n",
            "fr_model  82.034490   78.860075   27.519284  115.796956   80.100849   \n",
            "in_model  90.019762  107.158388   99.787939   33.680702  108.597459   \n",
            "it_model  73.982244   60.364464   63.619132   85.022807   27.949371   \n",
            "nl_model  67.660599   94.398034   77.002309   80.364834   97.015076   \n",
            "pt_model  98.459815   61.063432   80.949848  113.485056   72.657134   \n",
            "tl_model  83.726704  107.825760  105.862297   67.602114   97.840886   \n",
            "\n",
            "                  nl          pt          tl  \n",
            "en_model   61.422953  102.652615   65.454885  \n",
            "es_model   86.526790   55.849217   83.852921  \n",
            "fr_model   77.953623   91.331620  118.426125  \n",
            "in_model   87.721571  127.107799   68.318214  \n",
            "it_model   84.358450   75.133654   82.099920  \n",
            "nl_model   28.487105  115.947760   87.673987  \n",
            "pt_model  103.752383   30.097126  105.482377  \n",
            "tl_model   89.110721  120.048166   33.767835  \n",
            "DataFrame of perplexity for n=3, add_one =False:\n",
            "                  en          es          fr          in          it  \\\n",
            "en_model   10.921999  117.729975   81.424843   77.802859  105.134105   \n",
            "es_model  111.415093   10.430856   89.893183  115.334352   79.429330   \n",
            "fr_model  156.772223  150.140266   10.258980  198.968866  136.237585   \n",
            "in_model  152.079900  235.264782  167.552912   11.236029  238.260683   \n",
            "it_model   90.942859   70.817712   76.159034  101.551387    9.872314   \n",
            "nl_model  138.161507  232.550872  150.825478  124.758122  224.727889   \n",
            "pt_model  154.440048   80.220956  119.642507  143.885979   99.142703   \n",
            "tl_model  125.758773  172.312157  153.489255   82.925432  150.868592   \n",
            "\n",
            "                  nl          pt          tl  \n",
            "en_model   63.358218  154.734100   65.383332  \n",
            "es_model  116.115543   79.403854   98.254611  \n",
            "fr_model  132.113648  200.924519  215.240327  \n",
            "in_model  126.007682  328.942501   85.703104  \n",
            "it_model  103.388319  106.398754   90.691757  \n",
            "nl_model    9.703911  319.486263  155.074650  \n",
            "pt_model  148.746978   10.482324  131.793835  \n",
            "tl_model  123.267574  208.042607   10.566797  \n",
            "DataFrame of perplexity for n=4, add_one =True:\n",
            "                   en           es           fr           in           it  \\\n",
            "en_model    78.733155   887.020161   597.012289   641.677297   789.664708   \n",
            "es_model   817.120110    72.054280   569.311101   966.357176   490.483258   \n",
            "fr_model   886.813153   844.779753    68.026079  1359.963517   837.872468   \n",
            "in_model  1129.528048  1541.110730  1265.564558    92.703318  1529.770722   \n",
            "it_model   731.997278   498.179136   589.682137   881.633090    70.709354   \n",
            "nl_model   775.345529  1257.001259   873.610897   909.614623  1266.686780   \n",
            "pt_model  1065.354792   453.972990   765.996037  1163.699862   603.239126   \n",
            "tl_model   899.086276  1281.144110  1215.165588   615.092394  1080.809463   \n",
            "\n",
            "                   nl           pt           tl  \n",
            "en_model   477.804206  1190.211129   510.889164  \n",
            "es_model   866.727782   488.826962   836.847389  \n",
            "fr_model   824.835456  1228.514309  1462.962136  \n",
            "in_model   996.761352  2146.130707   678.120292  \n",
            "it_model   851.788357   780.805063   800.952029  \n",
            "nl_model    68.403505  1837.764607  1106.830677  \n",
            "pt_model  1118.477052    80.962554  1055.719830  \n",
            "tl_model   974.199133  1577.818697    89.480169  \n",
            "DataFrame of perplexity for n=4, add_one =False:\n",
            "                    en            es            fr           in            it  \\\n",
            "en_model      8.488455   2591.861576   1211.212425   884.316294   1782.970122   \n",
            "es_model   2716.254546      9.004851   1437.227275  3160.160816    706.684502   \n",
            "fr_model   2382.753492   2802.482428      7.726003  4394.742397   2407.595258   \n",
            "in_model  11633.622168  15928.973507  12339.906668     8.118135  14692.509873   \n",
            "it_model   2760.529296   1030.227672   1698.652395  2881.255871      7.237045   \n",
            "nl_model   3268.286773   9074.435136   5065.617327  4211.208571   9639.958883   \n",
            "pt_model   5362.962637    859.380604   2827.035476  5023.547981   1060.137840   \n",
            "tl_model   3951.586866   5754.215482   7134.035476  1143.417532   4517.739744   \n",
            "\n",
            "                   nl            pt           tl  \n",
            "en_model   600.713926   4629.574714   459.668992  \n",
            "es_model  3239.760657    965.821799  1607.298004  \n",
            "fr_model  2198.035327   6222.466420  4261.119459  \n",
            "in_model  7340.563701  31528.987103  1696.526247  \n",
            "it_model  3535.387435   2365.334840  1644.604490  \n",
            "nl_model     5.807919  20429.269629  5263.638807  \n",
            "pt_model  5962.053259     10.916707  2734.174224  \n",
            "tl_model  4630.612426   8928.564664     8.265946  \n"
          ]
        }
      ],
      "source": [
        "run_match()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg4h5Cl0q2nR"
      },
      "source": [
        "**Part 6**\n",
        "\n",
        "Each line in the file test.csv contains a sentence and the language it belongs to. We will write a function that uses your language models to classify the correct language of each sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qD6IRIQLrlZF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def eval_sentence(n, model, sentence):\n",
        "  # n - the n-gram that you used to build your model (must be the same number)\n",
        "  # model - the dictionary (model) to use for calculating perplexity\n",
        "  # data_file - the tweets file that you wish to claculate a perplexity score for\n",
        "  perplexity = 0\n",
        "\n",
        "  # pad each tweet with start and end symbols given n\n",
        "  start_symbol = '<s>'\n",
        "  end_symbol = '<e>'\n",
        "  processed_sentence = start_symbol*(n-1) + sentence + end_symbol\n",
        "\n",
        "\n",
        "  # Calculate the perplexity of the model on the sentence\n",
        "  log_prob = 0\n",
        "  N = 0\n",
        "\n",
        "  for i in range(n - 1, len(processed_sentence)):\n",
        "    prefix = processed_sentence[i - n + 1:i]\n",
        "    token = processed_sentence[i]\n",
        "    if prefix in model and token in model[prefix]:\n",
        "      if model[prefix][token] == 0:\n",
        "        log_prob += math.log((1e-10), 2)\n",
        "      else:\n",
        "        log_prob += math.log(model[prefix][token], 2)\n",
        "      N += 1\n",
        "    else:\n",
        "      log_prob += math.log((1e-10), 2)  # Smoothing for unknown tokens\n",
        "      N += 1\n",
        "\n",
        "  perplexity = 2 ** (-log_prob / (N - n + 1))\n",
        "\n",
        "  return perplexity\n",
        "\n",
        "\n",
        "def classify(n, models, test_csv):\n",
        "  reader = csv.reader(test_csv)\n",
        "  test_csv.seek(0)\n",
        "  predictions = []\n",
        "  languages = ['en', 'es', 'fr', 'in', 'it', 'nl', 'pt', 'tl']\n",
        "  for row in reader:\n",
        "    cur_perp = np.inf\n",
        "    best_perp = np.inf\n",
        "    best_language = None\n",
        "    language_pointer = 0\n",
        "    for model in models:\n",
        "      cur_perp = eval_sentence(n, model, row[1])\n",
        "      if cur_perp < best_perp:\n",
        "        best_perp = cur_perp\n",
        "        best_language = language_pointer\n",
        "      language_pointer += 1\n",
        "    predictions.append(languages[best_language])\n",
        "  return predictions\n",
        "\n",
        "def train_models(n, add_one):\n",
        "  csv_paths = ['/content/nlp-course/lm-languages-data-new/en.csv',\n",
        "            '/content/nlp-course/lm-languages-data-new/es.csv',\n",
        "            '/content/nlp-course/lm-languages-data-new/fr.csv',\n",
        "            '/content/nlp-course/lm-languages-data-new/in.csv',\n",
        "            '/content/nlp-course/lm-languages-data-new/it.csv',\n",
        "            '/content/nlp-course/lm-languages-data-new/nl.csv',\n",
        "            '/content/nlp-course/lm-languages-data-new/pt.csv',\n",
        "            '/content/nlp-course/lm-languages-data-new/tl.csv']\n",
        "  vocab = calc_vocab(csv_paths)\n",
        "\n",
        "\n",
        "  models = []\n",
        "  data = [ [] for i in range(8) ]   # ['en', 'es', 'fr', 'in', 'it', 'nl', 'pt', 'tl']\n",
        "  for i in range(8):\n",
        "    cur_model = lm(n, vocab, csv_paths[i], add_one)\n",
        "    models.append(cur_model)\n",
        "  return models\n",
        "\n",
        "def calc_acc(preds, test_csv):\n",
        "  reader = csv.reader(test_csv)\n",
        "  test_csv.seek(0)  #<-- set the iterator to beginning of the input file\n",
        "  labels = []\n",
        "  for row in reader:\n",
        "    labels.append(row[2])\n",
        "  count = 0\n",
        "  for i in range(len(labels)-1):\n",
        "    if preds[i+1] == labels[i+1]:\n",
        "      count +=1\n",
        "  return count / (len(labels) -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5ECmLd3rktZ"
      },
      "source": [
        "**Part 7**\n",
        "\n",
        "We will calculate the F1 score of your output from part 6. (hint: we can use https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html).\n",
        "\n",
        "Then we'll load the results to a CSV (using a DataFrame), where the row indicates the F1 results, and the columns indicate the model used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VOBO3YQls66r"
      },
      "outputs": [],
      "source": [
        "import sklearn.metrics\n",
        "\n",
        "def calc_f1(y_true,y_pred):\n",
        "\n",
        "  #F1 = 2 * (precision * recall) / (precision + recall)\n",
        "  F1 = sklearn.metrics.f1_score(y_true, y_pred, average = 'micro')\n",
        "\n",
        "  return F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3TmQynlpEEtb"
      },
      "outputs": [],
      "source": [
        "F1_list = [ [] for i in range(8) ] # ['en', 'es', 'fr', 'in', 'it', 'nl', 'pt', 'tl']\n",
        "lm_add_one = []\n",
        "lm_no_add_one = []\n",
        "\n",
        "test_csv = open('/content/nlp-course/lm-languages-data-new/test.csv')\n",
        "reader = csv.reader(test_csv)\n",
        "test_csv.seek(0)  #<-- set the iterator to beginning of the input file\n",
        "y_true = []\n",
        "for row in reader:\n",
        "  y_true.append(row[2])\n",
        "\n",
        "for n in range(0,4):\n",
        "  models_add_one = train_models(n+1, add_one=True)\n",
        "  lm_add_one.append(models_add_one)\n",
        "  models_no_add_one = train_models(n+1, add_one=False)\n",
        "  lm_no_add_one.append(models_add_one)\n",
        "  test_pred_add_one = classify(n+1, models_add_one, test_csv)\n",
        "  test_pred_no_add_one = classify(n+1, models_no_add_one, test_csv)\n",
        "\n",
        "  f1_score_add_one = calc_f1(y_true,test_pred_add_one)\n",
        "  f1_score_no_add_one = calc_f1(y_true,test_pred_no_add_one)\n",
        "\n",
        "  F1_list[n].append(f1_score_add_one)\n",
        "  F1_list[n+4].append(f1_score_no_add_one)\n",
        "\n",
        "\n",
        "f1_df = pd.DataFrame(F1_list, columns =['F1 on test.csv'] ,\n",
        "                    index=['n=1, add_one',\n",
        "                                        'n=2, add_one',\n",
        "                                        'n=3, add_one',\n",
        "                                        'n=4, add_one',\n",
        "                                        'n=1, no_add_one',\n",
        "                                        'n=2, no_add_one',\n",
        "                                        'n=3, no_add_one',\n",
        "                                        'n=4, no_add_one'])\n",
        "file_name = 'part7.csv'\n",
        "f1_df.to_csv(file_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfBYgfjADNPL"
      },
      "source": [
        "<br><br><br><br>\n",
        "**Part 8**  \n",
        "Let's use the Language model (dictionary) for generation (NLG).\n",
        "\n",
        "When it comes to sampling from a language model decoder during text generation, there are several different methods that can be used to control the randomness and diversity of the generated text.\n",
        "\n",
        "Some of the most commonly used methods include:\n",
        "\n",
        "> `Greedy sampling`\n",
        "In this method, the model simply selects the word with the highest probability as the next word at each time step. This method can produce fluent text, but it can also lead to repetitive or predictable output.\n",
        "\n",
        "> `Temperature scaling`  \n",
        "Temperature scaling involves scaling the logits output of the language model by a temperature parameter before softmax normalization. This has the effect of smoothing the distribution of probabilities and increasing the probability of lower-probability words, which can lead to more diverse and creative output.\n",
        "\n",
        "> `Top-K sampling`  \n",
        "In this method, the model restricts the sampling to the top-K most likely words at each time step, where K is a predefined hyperparameter. This can generate more diverse output than greedy sampling, while limiting the number of low-probability words that are sampled.\n",
        "\n",
        "> `Nucleus sampling` (also known as top-p sampling)  \n",
        "This method restricts the sampling to the smallest possible set of words whose cumulative probability exceeds a certain threshold, defined by a hyperparameter p. Like top-K sampling, this can generate more diverse output than greedy sampling, while avoiding sampling extremely low probability words.\n",
        "\n",
        "> `Beam search`  \n",
        "Beam search involves maintaining a fixed number k of candidate output sequences at each time step, and then selecting the k most likely sequences based on their probabilities. This can improve the fluency and coherence of the output, but may not produce as much diversity as sampling methods.\n",
        "\n",
        "The choice of sampling method depends on the specific application and desired balance between fluency, diversity, and randomness. Hyperparameters such as temperature, K, p, and beam size can also be tuned to adjust the behavior of the language model during sampling.\n",
        "\n",
        "\n",
        "We can read more about this concept in <a href='https://huggingface.co/blog/how-to-generate#:~:text=pad_token_id%3Dtokenizer.eos_token_id)-,Greedy%20Search,-Greedy%20search%20simply'>this</a> blog post.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "v4TrLs1kI3fW"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import heapq\n",
        "import copy\n",
        "\n",
        "def sample_greedy(probabilities, k=1):\n",
        "  pred = ''\n",
        "  for i in range(k):\n",
        "        max_char = max(probabilities, key=lambda key: probabilities[key]) # Select the characater with the highest probability\n",
        "        pred += max_char                                                  # Append the character to the prediction\n",
        "  return pred\n",
        "\n",
        "def sample_temperature(probabilities, temperature=1, k=1):\n",
        "  pred = ''\n",
        "  denominator = sum([math.exp(prob/temperature) for prob in probabilities.values()])\n",
        "  new_probs = {key: math.exp(prob/temperature) / denominator for key, prob in probabilities.items()}\n",
        "  for i in range(k):\n",
        "    next_char = random.choices(list(new_probs.keys()), weights=list(new_probs.values()))[0]  # Sample the characaters with the updated probabillities\n",
        "    pred += next_char                                                                        # Append the character to the prediction\n",
        "  return pred\n",
        "\n",
        "def sample_topK(probabilities, k=1):\n",
        "  pred = ''\n",
        "  k_highest = heapq.nlargest(k, probabilities, key=probabilities.get)         # Find the k highest prob values\n",
        "  new_probs = {key: probabilities[key] for key in k_highest}                  # Create a new dictionary with only the k largest key-value pairs\n",
        "  for i in range(k):\n",
        "    next_char = random.choices(list(new_probs.keys()), weights=list(new_probs.values()))[0]\n",
        "    pred += next_char\n",
        "  return pred\n",
        "\n",
        "def sample_topP(probabilities, p=0.8):\n",
        "  sorted_probs = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)   # Sort the dictionary in decending order\n",
        "  cumulative_probs = np.cumsum([prob for _, prob in sorted_probs])\n",
        "  candidates = [candidate for candidate, cum_prob in zip(sorted_probs, cumulative_probs) if cum_prob <= p]\n",
        "  if not candidates:                                                               # If no candidates have prob less that p we return the first largest Key\n",
        "    candidates = {(sorted_probs[0][0], sorted_probs[0][1])}\n",
        "  new_probs = {candidate: prob for candidate, prob in candidates}\n",
        "  next_char = random.choices(list(new_probs.keys()), weights=list(new_probs.values()))[0]\n",
        "  return next_char\n",
        "\n",
        "def sample_beam(n, model, vocab,beams, k=3):\n",
        "\n",
        "  temp_beams = []\n",
        "  for i in range(k):\n",
        "    for j in range(k):\n",
        "      temp_beams.append(copy.deepcopy(beams[i]))\n",
        "  for i, beam in enumerate(beams):\n",
        "    prefix_string = ''.join([elem[0] for elem in beam[0][-(n-1):]])           # select and collapse to the appropriate prefix length, this is needed since start_tokens length is unkown\n",
        "    beam_prob = model[prefix_string]\n",
        "    k_highest = heapq.nlargest(k, beam_prob, key=beam_prob.get)             # Find the k highest prob values\n",
        "    new_probs = {key: beam_prob[key] for key in k_highest}                  # Create a new dictionary with only the k largest key-value pairs\n",
        "    keys_list = list(new_probs)\n",
        "    probs_list = list(new_probs.values())\n",
        "\n",
        "    for j in range(len(keys_list)):\n",
        "      while True:\n",
        "        #print(f'j={j}, keys_list length is {len(keys_list)}')\n",
        "        if keys_list[j] == '<e>':\n",
        "          break\n",
        "        if keys_list[j] == 'unseen':                                           # if the predicted next char is 'unseen' then randomly select a char, make sure the prefix exists in the vocabulary\n",
        "          keys_list[j] = random.choice(vocab)[0]\n",
        "        cutted_prefix = beam[0][-(n-2):]\n",
        "        cutted_string = ''.join([elem[0] for elem in cutted_prefix])\n",
        "        if cutted_string + keys_list[j] in model.keys():\n",
        "          break\n",
        "        keys_list[j] = 'unseen'\n",
        "      temp_beams[i*k+j][0].append([keys_list[j]])\n",
        "      temp_beams[i*k+j][1] *= probs_list[j]\n",
        "\n",
        "\n",
        "  # Initializing the resulted k beams list:\n",
        "  resulted_beams = []\n",
        "   # finding the best k beams from temp_beams:\n",
        "  for i in range(k):\n",
        "    best_beam = None\n",
        "    best_prob = 0\n",
        "    for beam in temp_beams:\n",
        "      if beam[1] > best_prob:\n",
        "        best_prob = beam[1]\n",
        "        best_beam = beam\n",
        "    resulted_beams.append(best_beam)\n",
        "    temp_beams.remove(best_beam)\n",
        "\n",
        "  return resulted_beams\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Giylo6-lI21t"
      },
      "source": [
        "We will use your Language Model to generate each one out of the following examples with the coresponding params.    \n",
        "Notice the 4 core issues:\n",
        "- Starting tokens\n",
        "- Length of the generation\n",
        "- Sampling methond (use all)\n",
        "- Stop Token (if this token is sampled, stop generating)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9pUOkRtjN1mI"
      },
      "outputs": [],
      "source": [
        "test_ = {\n",
        "    'example1' : {\n",
        "        'start_tokens' : \"H\",\n",
        "        'sampling_method' : ['greedy','beam'],\n",
        "        'gen_length' : \"10\",\n",
        "        'stop_token' : \"\\n\",\n",
        "        'generation' : []\n",
        "    },\n",
        "    'example2' : {\n",
        "        'start_tokens' : \"H\",\n",
        "        'sampling_method' : ['temperature','topK','topP'],\n",
        "        'gen_length' : \"10\",\n",
        "        'stop_token' : \"\\n\",\n",
        "        'generation' : []\n",
        "    },\n",
        "    'example3' : {\n",
        "        'start_tokens' : \"He\",\n",
        "        'sampling_method' : ['greedy','beam','temperature','topK','topP'],\n",
        "        'gen_length' : \"20\",\n",
        "        'stop_token' : \"me\",\n",
        "        'generation' : []\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTbF-9zKVchQ"
      },
      "source": [
        "Now we will use the LM to generate a string based on the parametes of each examples, and store the generation sequance at the generation list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3zf-omUXQezz"
      },
      "outputs": [],
      "source": [
        "def gen_text(n, model, vocab, start_tokens, sampling_method, gen_length, stop_token):\n",
        "  generation = []\n",
        "  prefix = [vocab[0]] * n + [[c] for c in start_tokens]                     # pad the prefix with start tokens according the ngram length\n",
        "  prefix_string = ''.join([elem[0] for elem in prefix[-(n-1):]])            # select and collapse to the appropriate prefix length, this is needed since start_tokens length is unkown\n",
        "  probabilities = model[prefix_string]\n",
        "  next_char = ''\n",
        "  if sampling_method == sample_beam:\n",
        "    k = 3                                                                   # Deciding hyper-parameter k for sample_beam\n",
        "    prefix_beam = prefix[-(n-1):]                                           # pad the prefix with start tokens according the ngram length\n",
        "    beams = [[prefix_beam,1] for i in range(k)]\n",
        "    best_beam = 0\n",
        "    for i in range(gen_length):\n",
        "      next_beams = sample_beam(n,model,vocab,beams,k)\n",
        "      beams = next_beams\n",
        "    beams[0][0] = beams[0][0][n-1:]\n",
        "    text = ''.join([elem[0] for elem in beams[0][0][-(gen_length):]])\n",
        "  else:\n",
        "    for k in range(gen_length):\n",
        "      while True:\n",
        "        next_char = sampling_method(probabilities)\n",
        "        if next_char == 'unseen':                                              # if the predicted next char is 'unseen' then randomly select a char, make sure the prefix exists in the vocabulary\n",
        "          next_char = random.choice(vocab)[0]\n",
        "        prefix = prefix[1:] + [next_char]                                      # build new prefix\n",
        "        prefix_string = ''.join([elem[0] for elem in prefix[-(n-1):]])\n",
        "        if prefix_string in model.keys():\n",
        "          break\n",
        "      if [next_char] == vocab[1]:                                                # end generation if end token is predicted\n",
        "        break\n",
        "      generation.append(next_char)\n",
        "      probabilities = model[prefix_string]                                       # select new probabiliies\n",
        "    #print(f'generetion before text: {generation}')\n",
        "    text = ''.join([elem for elem in generation])\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3eihxDYPqkuD"
      },
      "outputs": [],
      "source": [
        "# Using the best english model for generation:\n",
        "\n",
        "english_model = lm(4, vocab, '/content/nlp-course/lm-languages-data-new/en.csv', True)\n",
        "en_vocab = calc_vocab(['/content/nlp-course/lm-languages-data-new/en.csv'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "j56NN2QnBOBF"
      },
      "outputs": [],
      "source": [
        "#Generating new text into \"test_\":\n",
        "\n",
        "for example_key, example_value in test_.items():\n",
        "    # iterate over each sampling method for the current example\n",
        "    for method in example_value['sampling_method']:\n",
        "        # generate text using the current method\n",
        "        sample_method = eval(\"sample_\"+method)\n",
        "        text = gen_text(4, english_model, vocab, example_value['start_tokens'], sample_method, int(example_value['gen_length']), example_value['stop_token'])\n",
        "        # append the generated text to the generation list for the current example and method\n",
        "        example_value['generation'].append(text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvla30-lVw8n",
        "outputId": "2d9e1010-101b-48cf-9027-f3fa482c6c1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------- NLG --------\n",
            "example1:\n",
            "\tgreedy >> Hey https:/\n",
            "\tbeam >> Hey https:/\n",
            "\n",
            "example2:\n",
            "\ttemperature >> Hedge. Risi\n",
            "\ttopK >> Hey https:/\n",
            "\ttopP >> Hertin \n",
            "\n",
            "example3:\n",
            "\tgreedy >> Hey https://t.co/FIGQk\n",
            "\tbeam >> Hey https://t.co/FIGQ\n",
            "\ttemperature >> Her:wemin .:: abraConn\n",
            "\ttopK >> Hey https://t.co/FIGQk\n",
            "\ttopP >> Hear wow may's a use m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('-------- NLG --------')\n",
        "\n",
        "for k,v in test_.items():\n",
        "  l = ''.join([f'\\t{sm} >> {v[\"start_tokens\"]}{g}\\n' for sm,g in zip(v['sampling_method'],v['generation'])])\n",
        "  print(f'{k}:')\n",
        "  print(l)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}